{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leonmkim/lerobot_tutorial/blob/main/lerobot_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff7eafe4",
      "metadata": {
        "id": "ff7eafe4"
      },
      "source": [
        "Adapted from https://colab.research.google.com/github/TrossenRobotics/aloha_docs/blob/main/docs/files/LeRobot_Notebook.ipynb\n",
        "# LeRobot Training Instruction Manual\n",
        "\n",
        "# üéâ Welcome to the LeRobot Training Notebook!\n",
        "\n",
        "This guide will help you set up and train a model on a cloud-based platform, such as **Google Colab**, using **LeRobot** with **Hugging Face**.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ö†Ô∏è **Disclaimers:**\n",
        "\n",
        "- **GPU Subscription**: üîë Make sure you have the appropriate subscription plan that provides access to the necessary GPU (e.g., **A100**, **T4**). Review pricing and benefits on the cloud provider's website before proceeding.\n",
        "  \n",
        "- **Checkpoint Requirement**: ‚è≥ If resuming training, ensure that you have the previous training checkpoint available in your session. Without the checkpoint, the training **cannot be resumed**.\n",
        "\n",
        "---\n",
        "\n",
        "## üìù **Important Instructions:**\n",
        "\n",
        "- **Run All Cells Together**: üîÑ It is recommended to run all the cells in one go if you plan to leave the session **unmonitored**. This helps avoid session timeouts or disruptions.\n",
        "\n",
        "- **GPU & Compute Units**: üéõÔ∏è Ensure you select a suitable GPU (e.g., **A100**, **T4**) and have enough compute units for your session. A typical 5-hour training session requires approximately **70 compute units**.\n",
        "\n",
        "- **Monitor Training**: üëÄ It‚Äôs advisable to monitor the **first few epochs** to ensure that the training is running smoothly before leaving the session unattended.\n",
        "\n",
        "- **Local Storage**: üíæ You will be prompted to choose whether you want to store the training outputs **locally** at the end of the process.\n",
        "\n",
        "---\n",
        "\n",
        "Now, let‚Äôs begin the setup process! üöÄ\n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73c63b0d",
      "metadata": {},
      "source": [
        "\n",
        "## (For Colab) GPU Setup & Compute Units\n",
        "\n",
        "Make sure runtime type is set to GPU (e.g. **A100**, **T4**).\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46a58617",
      "metadata": {
        "id": "46a58617"
      },
      "source": [
        "\n",
        "## Installing Dependencies\n",
        "\n",
        "In this step, we'll install all the necessary dependencies for running LeRobot and performing model training.\n",
        "\n",
        "For installing rerun, kernel will be restarted after pip installation.\n",
        "\n",
        "Ensure that these packages are successfully installed before proceeding to the next steps.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "49888a14",
      "metadata": {
        "id": "49888a14"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "try:\n",
        "    import google.colab\n",
        "    IS_COLAB = True\n",
        "except ImportError:\n",
        "    IS_COLAB = False\n",
        "    home_env_var = os.environ[\"HOME\"]\n",
        "    ld_library_path_env_var = os.environ[\"LD_LIBRARY_PATH\"]\n",
        "\n",
        "root_dir = \"/content\" if IS_COLAB else os.path.expanduser(\"~/lerobot_tutorial\")\n",
        "custom_scripts_dirpath = os.path.join(root_dir, \"lerobot_tutorial\") if IS_COLAB else root_dir\n",
        "lerobot_root_dir = os.path.join(root_dir, \"lerobot\")\n",
        "train_output_dir = os.path.join(lerobot_root_dir, \"outputs\", \"train\")\n",
        "visualize_dataset_path = os.path.join(root_dir, \"lerobot_tutorial\", \"visualize_dataset.py\") if IS_COLAB else os.path.join(root_dir, \"visualize_dataset.py\")\n",
        "sys.path.append(custom_scripts_dirpath)\n",
        "\n",
        "if IS_COLAB:\n",
        "    print(\"Installing required dependencies...\")\n",
        "\n",
        "    !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 # specifically 12.1 as 12.4 has issues locally\n",
        "    !pip install --upgrade blinker wandb datasets huggingface-hub hydra-core gitpython flask diffusers InquirerPy\n",
        "\n",
        "    # Install blinker if needed\n",
        "    !pip install --ignore-installed blinker\n",
        "\n",
        "    # clone custom visualize dataset from tutorial repo\n",
        "    if not os.path.exists(visualize_dataset_path):\n",
        "      !git clone https://github.com/leonmkim/lerobot_tutorial.git\n",
        "    \n",
        "    # Install LeRobot repository\n",
        "    if not os.path.exists(lerobot_root_dir):\n",
        "        !git clone https://github.com/huggingface/lerobot.git\n",
        "    %cd {lerobot_root_dir}\n",
        "    !pip install -e .[pusht]\n",
        "    \n",
        "    # install custom version of gym-aloha that supports varying initialization distribution\n",
        "    %cd {root_dir}\n",
        "    !git clone https://github.com/leonmkim/gym-aloha.git\n",
        "    %cd gym-aloha\n",
        "    !pip install -e . \n",
        "\n",
        "    try: \n",
        "        import rerun as rr\n",
        "    except ImportError:\n",
        "        print(\"The `rerun` module is missing. Installing now via pip...\")\n",
        "        !pip install --upgrade rerun-sdk[notebook]\n",
        "        print('Installation completed. The runtime needs to be restarted. Stopping now.')\n",
        "        os.kill(os.getpid(), 9)\n",
        "\n",
        "    print(\"Dependencies installed successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f623b1c9",
      "metadata": {},
      "source": [
        "\n",
        "## Hugging Face Login & Dataset Setup\n",
        "\n",
        "We will now log into Hugging Face using the token provided. After login, the dataset repo, job name, and output directory that you specified will be configured for the training session.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b6e6f38",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hugging Face login token\n",
        "print(\"Generate a Hugging Face token from: https://huggingface.co/settings/tokens\")\n",
        "hf_token = input(\"Please enter your Hugging Face token: \")\n",
        "\n",
        "# Log in to Hugging Face and verify login\n",
        "print(\"Logging into Hugging Face...\")\n",
        "!huggingface-cli login --token {hf_token}\n",
        "\n",
        "# Verify the login by checking the user information\n",
        "user_info = !huggingface-cli whoami\n",
        "print(f\"Logged in as: {user_info[0]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9ba80b6",
      "metadata": {},
      "source": [
        "\n",
        "## WandB login\n",
        "\n",
        "Weights & Biases (WandB) is a tool that helps track and visualize various metrics during model training. We will log in now to enable tracking.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2328797b",
      "metadata": {},
      "outputs": [],
      "source": [
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89cbc43f",
      "metadata": {},
      "source": [
        "\n",
        "# ACT/ALOHA Transfer Cube Example\n",
        "\n",
        "In this example, we will train an [Action Chunking Transformer (ACT) policy](https://tonyzhaozh.github.io/aloha/) on a cube transfer task using the [ALOHA bimanual robot](https://www.trossenrobotics.com/aloha-kits).\n",
        "\n",
        "## What is ACT?\n",
        "\n",
        "ACT is a state-of-the-art imitation learning algorithm that leverages the transformer architecture to generate actions given sensor observations (e.g. joint encoders, camera images, etc.). Unlike more naive imitation learning methods, ACT predicts action sequences multiple timesteps into the future (coined \"action chunks\") at each inference step, with the aim of reducing the effective horizon of a task and thus mitigating compounding error. Furthermore, action chunks are aggregated across multiple inference steps through \"temporal ensembling\" to smooth out potential discontinuities in the predicted action sequences. Details on the ACT algorithm can be found in the [original paper](https://arxiv.org/abs/2304.13705).\n",
        "\n",
        "![image](ACT.png \"ACT figure\")\n",
        "\n",
        "## What is ALOHA?\n",
        "\n",
        "Where do we get the data for training? A Low-cost Open-source Hardware System for Bimanual Teleoperation (ALOHA) includes \"leader\" arms that can be naturally operated by a human demonstrator with \"follower\" arms that track the joint angles of the leader arms through PID control. \n",
        "\n",
        "<!-- link video -->\n",
        "\n",
        "<iframe width=\"700\" height=\"394\" src=\"https://www.youtube.com/embed/PHXQFE-Rteo?si=QOFaoEHHAez-ByqK\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n",
        "\n",
        "## Transfer Cube Task\n",
        "In this simple simulated example, we will train on pre-recorded demonstrations of transferring a red cube from one arm to the other. The red cube is randomly initialized in the workspace and the task is considered successful if 1) the cube is grasped off the table by the right arm and 2) the cube is then grasped by the left arm.\n",
        "\n",
        "<!-- link video -->\n",
        "<video src=\"ALOHA_transfer_cube_success.mp4\" width=\"640\" height=\"480\" controls></video>\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65853dae",
      "metadata": {},
      "source": [
        "\n",
        "## Configure Settings\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f76587ef",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset and job details\n",
        "dataset_repo_id = \"lerobot/aloha_sim_transfer_cube_human\"\n",
        "task_id = \"AlohaTransferCube-v0\"\n",
        "training_offline_steps = 2000 # short training in class \n",
        "training_eval_freq = 1000\n",
        "training_save_freq = 1000\n",
        "training_log_freq = 250\n",
        "# training_offline_steps = 200000\n",
        "# training_eval_freq = 20000\n",
        "# training_save_freq = 20000\n",
        "eval_n_episodes = 50\n",
        "eval_batch_size = 50\n",
        "\n",
        "print(\"**Important**: Use a valid directory/jobs name. Avoid numbers or special characters other than '_'.\")\n",
        "print(\"Example: 'training_results_aloha' or 'aloha_training_output'\")\n",
        "\n",
        "job_name = \"train_aloha_sim_transfer_cube_human_200k\"\n",
        "\n",
        "# Output directory with naming format instructions\n",
        "\n",
        "output_dir = job_name\n",
        "\n",
        "# Resume flag with disclaimer\n",
        "# resume_flag = input(\"Do you want to resume training from a previous checkpoint? (yes/no): \")\n",
        "resume_flag = \"no\"\n",
        "resume_cmd = \"--resume\" if resume_flag.lower() == 'yes' else \"\"\n",
        "\n",
        "# Model upload flag\n",
        "# upload_choice = input(\"Do you want to upload the model to Hugging Face after training? (yes/no): \")\n",
        "upload_choice = \"yes\"\n",
        "model_repo_id = \"\"\n",
        "if upload_choice.lower() == 'yes':\n",
        "    # model_repo_id = input(\"Please enter the model repo ID to store your trained model to Hugging Face (e.g., TrossenRoboticsCommunity/aloha_static_logo_assembly): \")\n",
        "    model_repo_id = job_name\n",
        "\n",
        "# Local storage flag\n",
        "# store_locally = input(\"Do you want to store the training outputs locally? (yes/no): \")\n",
        "store_locally = \"yes\"\n",
        "\n",
        "# "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33d7288f",
      "metadata": {},
      "source": [
        "\n",
        "## Visualize Dataset\n",
        "\n",
        "Let's visualize the demonstrations to get a better understanding of the data we will be training on. The dataset consists of 50 human demonstrations of the cube transfer task where the red cube has been randomly initialized in the workspace. \n",
        "\n",
        "We will make use of \"[rerun](https://rerun.io/)\", a logging and visualization toolkit for multimodal (e.g. video, audio, text) data.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "197b275c",
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = LeRobotDataset(dataset_repo_id, root=None, local_files_only=False)\n",
        "visualize_dataset(\n",
        "    dataset=dataset,\n",
        "    episode_index=0, # change this to visualize a different episode\n",
        "    mode=rerun_mode,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70b98dd2",
      "metadata": {},
      "source": [
        "## Model Training or Resumption\n",
        "\n",
        "Now that everything is set up, we can either begin training the model or resume training from the last checkpoint, depending on your input.\n",
        "\n",
        "If resuming, make sure the checkpoint is available in your session. The training will continue from the last checkpoint if found.\n",
        "\n",
        "> **‚ö†Ô∏è Important: GPU Usage**\n",
        ">\n",
        "> By default, the training is configured to use a **GPU** for faster computation. If the runtime does not have access to a GPU, the training will fail.\n",
        ">\n",
        "> To avoid this issue:\n",
        ">\n",
        "> - **Ensure GPU is enabled** in your Colab runtime. You can check this by navigating to **Runtime > Change runtime type > Hardware accelerator** and selecting **GPU**.\n",
        "> - If you prefer to use a **CPU** instead, update the `device` argument to `device=cpu` in the training command in the next cell.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a19c3dd3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start or resume training depending on user choice\n",
        "%cd {lerobot_root_dir}\n",
        "if resume_flag.lower() == \"no\":\n",
        "    print(f\"Starting new training on {dataset_repo_id}...\")\n",
        "    # for sim\n",
        "    !python lerobot/scripts/train.py \\\n",
        "        dataset_repo_id={dataset_repo_id} \\\n",
        "        env=aloha \\\n",
        "        env.task={task_id} \\\n",
        "        policy=act \\\n",
        "        training.eval_freq={training_eval_freq} \\\n",
        "        training.log_freq={training_log_freq} \\\n",
        "        training.offline_steps={training_offline_steps} \\\n",
        "        training.save_freq={training_save_freq} \\\n",
        "        eval.n_episodes={eval_n_episodes} \\\n",
        "        eval.batch_size={eval_batch_size} \\\n",
        "        hydra.run.dir=outputs/train/{output_dir} \\\n",
        "        hydra.job.name={job_name} \\\n",
        "        device=cuda wandb.enable=true\n",
        "else:\n",
        "    print(f\"Resuming training from {output_dir}... (ensure checkpoint is available)\")\n",
        "    !python lerobot/scripts/train.py hydra.run.dir={output_dir} resume=true\n",
        "\n",
        "# step: optimization steps, smpl: num samples seen, ep: num episodes seen, epch: num epochs seen, Sigma rwrd: return, success: success rate, eval_s: \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb9508ec",
      "metadata": {},
      "source": [
        "\n",
        "## Eval policy\n",
        "\n",
        "Now let's evaluate the trained policy on the cube transfer task.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0978b40",
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd {lerobot_root_dir}\n",
        "!python lerobot/scripts/eval.py -p outputs/train/{output_dir}/checkpoints/last/pretrained_model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a0ed414",
      "metadata": {},
      "source": [
        "\n",
        "## Uploading the Model (Recommended)\n",
        "\n",
        "Once the model is trained, you can choose to upload it to Hugging Face for safekeeping. This is **highly recommended** if you are running long sessions or training a valuable model.\n",
        "\n",
        "Uploading the model will help protect against potential session interruptions or failures.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f254b02e",
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd {lerobot_root_dir}\n",
        "\n",
        "print(model_repo_id)\n",
        "# Model upload step if chosen\n",
        "if upload_choice.lower() == \"yes\":\n",
        "    print(\"Uploading the model to Hugging Face...\")\n",
        "    !huggingface-cli upload {model_repo_id}  outputs/train/{output_dir}/checkpoints/last/pretrained_model\n",
        "    print(\"Model uploaded to Hugging Face successfully.\")\n",
        "else:\n",
        "    print(\"Model upload skipped.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37a22835",
      "metadata": {},
      "source": [
        "\n",
        "## Eval policy: Skip to pre-trained results\n",
        "\n",
        "The model hasn't converged yet, so we'll skip to what you should get after a few hours of training (200k steps). \n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0eb578e2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# view training results for 200k steps\n",
        "%wandb serialexperimentsleon/lerobot/runs/onwbxyfw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aeb30721",
      "metadata": {},
      "outputs": [],
      "source": [
        "# skip to pretrained model\n",
        "%cd {lerobot_root_dir}\n",
        "# !python lerobot/scripts/eval.py -p lerobot/act_aloha_sim_transfer_cube_human\n",
        "!python lerobot/scripts/eval.py -p serialexperimentsleon/train_aloha_sim_transfer_cube_human_200k"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff364a5c",
      "metadata": {},
      "source": [
        "## Limitations of Imitation Learning: Distribution Shift\n",
        "\n",
        "With enough demonstrations, imitation learning can be a powerful tool for learning complex behaviors. However, in this simple example, we will see what happens when we test the trained policy on a cube that is initialized outside the nominal initialization range (described below). What do you think will happen in this case? How would you address this?\n",
        "\n",
        "![image](ALOHA_transfer_cube_init.png \"ALOHA_transfer_cube_init\")\n",
        "\n",
        "```python\n",
        "# nominal cube initialization range\n",
        "x_range = [0.0, 0.2]\n",
        "y_range = [0.4, 0.6]\n",
        "z_range = [0.05, 0.05]\n",
        "\n",
        "'''\n",
        "      ^\n",
        "      |\n",
        "      y\n",
        "      |\n",
        "Where o---x--->\n",
        "'''\n",
        "```\n",
        "\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db28f0a1",
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd {lerobot_root_dir}\n",
        "xrange=\"'[0.0,0.2]'\"\n",
        "# yrange=\"'[0.4,0.6]'\"\n",
        "yrange=\"'[0.6,0.7]'\" # disjoint from the nominal range\n",
        "zrange=\"'[0.05,0.05]'\"\n",
        "\n",
        "# !python lerobot/scripts/eval.py -p lerobot/act_aloha_sim_transfer_cube_human +env.gym.cube_init_xrange={xrange} +env.gym.cube_init_yrange={yrange} +env.gym.cube_init_zrange={zrange}\n",
        "!python lerobot/scripts/eval.py -p serialexperimentsleon/train_aloha_sim_transfer_cube_human_200k +env.gym.cube_init_xrange={xrange} +env.gym.cube_init_yrange={yrange} +env.gym.cube_init_zrange={zrange}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e46c5ea6",
      "metadata": {},
      "source": [
        "\n",
        "## Safeguarding Session Data and Local Storage\n",
        "\n",
        "To prevent data loss in case of session termination, you can zip the output directory and download it locally. If you selected local storage, the outputs will be saved to your local machine.\n",
        "\n",
        "Make sure to run this step to save all training outputs before closing your session.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb3ff900",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Zip the output directory and download it if local storage is chosen\n",
        "%cd {train_output_dir}\n",
        "!ls\n",
        "if IS_COLAB:\n",
        "    if store_locally.lower() == \"yes\":\n",
        "        print(\"Zipping outputs for download...\")\n",
        "        !zip -r trained.zip {output_dir}\n",
        "\n",
        "        # Download the zipped file\n",
        "        from google.colab import files\n",
        "        files.download('/content/lerobot/outputs/train/trained.zip')\n",
        "    else:\n",
        "        print(\"Local storage not selected, skipping download.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba7bbcc4",
      "metadata": {
        "id": "ba7bbcc4"
      },
      "source": [
        "\n",
        "# Troubleshooting and Recommendations\n",
        "\n",
        "1. **GPU Availability**: Ensure the selected GPU is available on your cloud platform (e.g., Colab).\n",
        "2. **Compute Units**: Ensure you have sufficient compute units. Each 5-hour session requires ~70 units.\n",
        "3. **Hugging Face Token**: You can generate a token [here](https://huggingface.co/settings/tokens).\n",
        "4. **Session Safeguards**: Always download your results (output files) to prevent data loss if the session terminates.\n",
        "5. **Checkpoint Reminder**: If resuming training, ensure that the checkpoint file from the previous session is present in the session.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7989c59",
      "metadata": {},
      "source": [
        "\n",
        "# EXTRA: Diffusion Policy/Push T Example\n",
        "\n",
        "TODO\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85f0be2b",
      "metadata": {},
      "source": [
        "\n",
        "## Configure Settings\n",
        "TODO\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d8c5c40",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect all necessary inputs from the user\n",
        "\n",
        "# Dataset and job details\n",
        "# dataset_repo_id = input(\"Please enter the dataset repo ID from Hugging Face (e.g., TrossenRoboticsCommunity/aloha_static_logo_assembly): \")\n",
        "dataset_repo_id = \"lerobot/pusht_keypoints\"\n",
        "task_id = \"PushT-v0\"\n",
        "training_offline_steps = 200000\n",
        "# training_offline_steps = 1000\n",
        "training_eval_freq = 20000\n",
        "training_save_freq = 20000\n",
        "training_log_freq = 50\n",
        "eval_n_episodes = 50\n",
        "eval_batch_size = 50\n",
        "\n",
        "print(\"**Important**: Use a valid directory/jobs name. Avoid numbers or special characters other than '_'.\")\n",
        "print(\"Example: 'training_results_aloha' or 'aloha_training_output'\")\n",
        "\n",
        "job_name = \"train_diffusion_pusht_keypoints\"\n",
        "\n",
        "# Output directory with naming format instructions\n",
        "\n",
        "output_dir = job_name\n",
        "\n",
        "# Resume flag with disclaimer\n",
        "# resume_flag = input(\"Do you want to resume training from a previous checkpoint? (yes/no): \")\n",
        "resume_flag = \"no\"\n",
        "resume_cmd = \"--resume\" if resume_flag.lower() == 'yes' else \"\"\n",
        "\n",
        "# Model upload flag\n",
        "# upload_choice = input(\"Do you want to upload the model to Hugging Face after training? (yes/no): \")\n",
        "upload_choice = \"yes\"\n",
        "model_repo_id = \"\"\n",
        "if upload_choice.lower() == 'yes':\n",
        "    # model_repo_id = input(\"Please enter the model repo ID to store your trained model to Hugging Face (e.g., TrossenRoboticsCommunity/aloha_static_logo_assembly): \")\n",
        "    model_repo_id = job_name\n",
        "\n",
        "# Local storage flag\n",
        "# store_locally = input(\"Do you want to store the training outputs locally? (yes/no): \")\n",
        "store_locally = \"yes\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a10c9082",
      "metadata": {},
      "source": [
        "\n",
        "## Visualize Dataset\n",
        "\n",
        "TODO\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27dc333a",
      "metadata": {},
      "outputs": [],
      "source": [
        "rerun_mode = \"notebook\" if IS_COLAB else \"local\"\n",
        "\n",
        "from lerobot.common.datasets.lerobot_dataset import LeRobotDataset\n",
        "from visualize_dataset import visualize_dataset\n",
        "\n",
        "dataset = LeRobotDataset('lerobot/pusht', root=None, local_files_only=False)\n",
        "visualize_dataset(\n",
        "    dataset=dataset,\n",
        "    episode_index=0,\n",
        "    mode=rerun_mode,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00808ca0",
      "metadata": {},
      "source": [
        "> **‚ö†Ô∏è Important Notice:**\n",
        ">\n",
        "> Before you start the training, make sure to edit the `act_aloha_real.yaml` file located at:\n",
        ">\n",
        "> **Click Here** >> `/content/lerobot/lerobot/configs/policy/act_aloha_real.yaml`\n",
        ">\n",
        "> This file contains crucial parameters such as `batch_size`, `offline_steps`, and `learning_rate`. You should update these parameters based on your training needs. For example, you can modify:\n",
        ">\n",
        "> - **Batch Size** (`training.batch_size`): Adjust the number of samples processed in each training step.\n",
        "> - **Offline Training Steps** (`training.offline_steps`): Define how many steps to run during offline training.\n",
        "> - **Learning Rate** (`training.lr`): Set the learning rate to control how quickly the model learns.\n",
        ">\n",
        "> Once the file is updated, you can proceed with training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43320c9c",
      "metadata": {},
      "source": [
        "## Model Training or Resumption\n",
        "\n",
        "Now that everything is set up, we can either begin training the model or resume training from the last checkpoint, depending on your input.\n",
        "\n",
        "If resuming, make sure the checkpoint is available in your session. The training will continue from the last checkpoint if found.\n",
        "\n",
        "> **‚ö†Ô∏è Important: GPU Usage**\n",
        ">\n",
        "> By default, the training is configured to use a **GPU** for faster computation. If the runtime does not have access to a GPU, the training will fail.\n",
        ">\n",
        "> To avoid this issue:\n",
        ">\n",
        "> - **Ensure GPU is enabled** in your Colab runtime. You can check this by navigating to **Runtime > Change runtime type > Hardware accelerator** and selecting **GPU**.\n",
        "> - If you prefer to use a **CPU** instead, update the `device` argument to `device=cpu` in the training command in the next cell.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "574e2295",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start or resume training depending on user choice\n",
        "%cd {lerobot_root_dir}\n",
        "if resume_flag.lower() == \"no\":\n",
        "    print(f\"Starting new training on {dataset_repo_id}...\")\n",
        "    # for sim\n",
        "    !python lerobot/scripts/train.py \\\n",
        "        dataset_repo_id={dataset_repo_id} \\\n",
        "        env=pusht \\\n",
        "        env.task={task_id} \\\n",
        "        env.gym.obs_type=environment_state_agent_pos \\\n",
        "        policy=diffusion_pusht_keypoints \\\n",
        "        training.eval_freq={training_eval_freq} \\\n",
        "        training.log_freq={training_log_freq} \\\n",
        "        training.offline_steps={training_offline_steps} \\\n",
        "        training.save_freq={training_save_freq} \\\n",
        "        eval.n_episodes={eval_n_episodes} \\\n",
        "        eval.batch_size={eval_batch_size} \\\n",
        "        hydra.run.dir=outputs/train/{output_dir} \\\n",
        "        hydra.job.name={job_name} \\\n",
        "        device=cuda wandb.enable=true \\\n",
        "        use_amp=true\n",
        "else:\n",
        "    print(f\"Resuming training from {output_dir}... (ensure checkpoint is available)\")\n",
        "    !python lerobot/scripts/train.py hydra.run.dir={output_dir} resume=true\n",
        "\n",
        "# step: optimization steps, smpl: num samples seen, ep: num episodes seen, epch: num epochs seen, Sigma rwrd: return, success: success rate, eval_s: \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03ee50f0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# view training results for 200k steps\n",
        "%wandb serialexperimentsleon/lerobot/runs/7j3hkdkq"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94d8bf11",
      "metadata": {},
      "source": [
        "\n",
        "## Eval policy\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7328b675",
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd {lerobot_root_dir}\n",
        "!python lerobot/scripts/eval.py -p outputs/train/{output_dir}/checkpoints/last/pretrained_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a9900a0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# skip to pretrained model\n",
        "%cd {lerobot_root_dir}\n",
        "!python lerobot/scripts/eval.py -p lerobot/diffusion_pusht_keypoints"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a00a24eb",
      "metadata": {},
      "source": [
        "\n",
        "## Uploading the Model (Recommended)\n",
        "\n",
        "Once the model is trained, you can choose to upload it to Hugging Face for safekeeping. This is **highly recommended** if you are running long sessions or training a valuable model.\n",
        "\n",
        "Uploading the model will help protect against potential session interruptions or failures.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe7d5496",
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd {lerobot_root_dir}\n",
        "\n",
        "print(model_repo_id)\n",
        "# Model upload step if chosen\n",
        "if upload_choice.lower() == \"yes\":\n",
        "    print(\"Uploading the model to Hugging Face...\")\n",
        "    !huggingface-cli upload {model_repo_id}  outputs/train/{output_dir}/checkpoints/last/pretrained_model\n",
        "    print(\"Model uploaded to Hugging Face successfully.\")\n",
        "else:\n",
        "    print(\"Model upload skipped.\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "machine_shape": "hm",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "lerobot_tutorial",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
